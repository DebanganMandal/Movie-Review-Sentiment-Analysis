{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import string\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading doc to memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r') #read only\n",
    "    text = file.read() # reading all text\n",
    "    file.close() #close the file\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(doc):\n",
    "    #split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #removing othen non alphabetic tokens\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    #filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    #filter out short tokens\n",
    "    tokens = [w for w in tokens if len(w)>1]\n",
    "    #the list of token has \n",
    "    return tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "#define vocab\n",
    "vocab = Counter()\n",
    "#add all docs to vocab\n",
    "process_docs('txt_sentoken/neg', vocab, True)\n",
    "process_docs('txt_sentoken/pos', vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "'''We can step through the vocabulary and remove all words that have a\n",
    "low occurrence, such as only being used once or twice in all reviews.'''\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if c>=min_occurance]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab.txt can be saved and used later for review prediction prior to encode them for modelling\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    #convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn a doc into clean tokens based on words in vocab\n",
    "def clean_vdoc(doc, vocab):\n",
    "    token = doc.split()\n",
    "    #remove punctuatuion from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    token = [w.translate(table) for w in token]\n",
    "    #filter out token not in vocab\n",
    "    token = [w for w in token if w in vocab]\n",
    "    token = ' '.join(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs in a directory\n",
    "def process_vdocs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    #walk through all the files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        #create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        #load the doc\n",
    "        doc = load_doc(path)\n",
    "        #clean doc\n",
    "        tokens = clean_vdoc(doc, vocab)\n",
    "        #add to the list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "#load all training reviews\n",
    "positive_docs = process_vdocs('txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_vdocs('txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "#create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "import tensorflow as tf\n",
    "seq = tf.keras.preprocessing.sequence\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = seq.pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training labels\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all test reviews\n",
    "positive_docs = process_vdocs('txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_vdocs('txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = seq.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = np.array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now ready to define our neural network model. Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use a Convolutional Neural Network (CNN) as they have proven\\nto be successful at document classification problems.\\nA conservative CNN configuration is used with 32 filters (parallel fields for processing words)\\nand a kernel size of 8 with a rectified linear (‘relu’) activation function.\\nThis is followed by a pooling layer that reduces the output of the convolutional layer\\nby half.\\n\\nNext, the 2D output from the CNN part of the model is flattened to one long 2D\\nvector to represent the ‘features’ extracted by the CNN. The back-end of the\\nmodel is a standard Multilayer Perceptron layers to interpret the CNN features.\\nThe output layer uses a sigmoid activation function to output a value between\\n0 and 1 for the negative and positive sentiment in the review.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We use a Convolutional Neural Network (CNN) as they have proven\n",
    "to be successful at document classification problems.\n",
    "A conservative CNN configuration is used with 32 filters (parallel fields for processing words)\n",
    "and a kernel size of 8 with a rectified linear (‘relu’) activation function.\n",
    "This is followed by a pooling layer that reduces the output of the convolutional layer\n",
    "by half.\n",
    "\n",
    "Next, the 2D output from the CNN part of the model is flattened to one long 2D\n",
    "vector to represent the ‘features’ extracted by the CNN. The back-end of the\n",
    "model is a standard Multilayer Perceptron layers to interpret the CNN features.\n",
    "The output layer uses a sigmoid activation function to output a value between\n",
    "0 and 1 for the negative and positive sentiment in the review.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define model \n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, Conv1D, MaxPooling1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length = max_length))\n",
    "model.add(Conv1D(filters = 32, kernel_size = 8, activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 8s - loss: 0.6903 - accuracy: 0.5150\n",
      "Epoch 2/10\n",
      "57/57 - 8s - loss: 0.6172 - accuracy: 0.6611\n",
      "Epoch 3/10\n",
      "57/57 - 7s - loss: 0.2480 - accuracy: 0.9161\n",
      "Epoch 4/10\n",
      "57/57 - 7s - loss: 0.0257 - accuracy: 0.9956\n",
      "Epoch 5/10\n",
      "57/57 - 7s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 7s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 7s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 7s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 7s - loss: 8.5853e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 7s - loss: 7.0608e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21635eb5988>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#complie network\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#fitting the model\n",
    "model.fit(Xtrain, ytrain, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "#accuracy check\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose = 0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Word2Vec method of Word embedding, much more popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 58109\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25767\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train word2vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1313, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 656, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 83968)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 83969     \n",
      "=================================================================\n",
      "Total params: 2,724,897\n",
      "Trainable params: 148,097\n",
      "Non-trainable params: 2,576,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 6s - loss: 0.7054 - accuracy: 0.5467\n",
      "Epoch 2/10\n",
      "57/57 - 6s - loss: 0.6628 - accuracy: 0.5922\n",
      "Epoch 3/10\n",
      "57/57 - 6s - loss: 0.5836 - accuracy: 0.7017\n",
      "Epoch 4/10\n",
      "57/57 - 6s - loss: 0.4714 - accuracy: 0.7872\n",
      "Epoch 5/10\n",
      "57/57 - 6s - loss: 0.3369 - accuracy: 0.8672\n",
      "Epoch 6/10\n",
      "57/57 - 6s - loss: 0.2320 - accuracy: 0.9328\n",
      "Epoch 7/10\n",
      "57/57 - 6s - loss: 0.1289 - accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "57/57 - 6s - loss: 0.0761 - accuracy: 0.9944\n",
      "Epoch 9/10\n",
      "57/57 - 6s - loss: 0.0415 - accuracy: 0.9978\n",
      "Epoch 10/10\n",
      "57/57 - 6s - loss: 0.0253 - accuracy: 1.0000\n",
      "Test Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
